{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "8ed7d1e5-2d5f-43b4-dec2-edc45092c75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GeneratedReviewsDetection'...\n",
            "remote: Enumerating objects: 278, done.\u001b[K\n",
            "remote: Counting objects: 100% (178/178), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 278 (delta 101), reused 77 (delta 38), pack-reused 100\u001b[K\n",
            "Receiving objects: 100% (278/278), 132.73 MiB | 36.34 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/congruiyS2023/GeneratedReviewsDetection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r GeneratedReviewsDetection/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL-qLL3hBk_m",
        "outputId": "f60cdaed-eec9-4338-dcae-25c216ed3143"
      },
      "id": "vL-qLL3hBk_m",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.0 (from -r GeneratedReviewsDetection/requirements.txt (line 1))\n",
            "  Downloading transformers-4.25.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.12 (from -r GeneratedReviewsDetection/requirements.txt (line 2))\n",
            "  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==0.22 (from -r GeneratedReviewsDetection/requirements.txt (line 3))\n",
            "  Downloading scikit-learn-0.22.tar.gz (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==1.3.5 (from -r GeneratedReviewsDetection/requirements.txt (line 4))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xgboost==1.7.3 (from -r GeneratedReviewsDetection/requirements.txt (line 5))\n",
            "  Downloading xgboost-1.7.3-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.10.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.1, 0.15.2, 0.16.0, 0.16.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext==0.10.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.25.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2QEVeI5Bo_p",
        "outputId": "fda407a1-fb62-49b4-86a2-a1b5b6071786"
      },
      "id": "Y2QEVeI5Bo_p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.0\n",
            "  Using cached transformers-4.25.0-py3-none-any.whl (5.8 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (2023.7.22)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.25.0 at https://files.pythonhosted.org/packages/2b/a6/b32ba581c064276dc254a4c2bdf5301a942fa28a4a9b41209d8909da37d2/transformers-4.25.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.7.0))\n",
            "Reason for being yanked: Version was not properly set\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8sGZewlrbLf",
        "outputId": "3cefd042-d93c-4865-abd1-1e96d9cc9723"
      },
      "id": "U8sGZewlrbLf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/GeneratedReviewsDetection && mkdir data\n",
        "!cd /content/GeneratedReviewsDetection && unzip \"./Data/Chinese Reviews/labeled_chinese_reviews_train.csv.zip\" -d ./data && mv ./data/labeled_chinese_reviews_train.csv ./data/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NugvJ3OFBsS0",
        "outputId": "cea9c20d-c522-42b3-907d-f6940f7ba9e5"
      },
      "id": "NugvJ3OFBsS0",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./Data/Chinese Reviews/labeled_chinese_reviews_train.csv.zip\n",
            "  inflating: ./data/labeled_chinese_reviews_train.csv  \n",
            "  inflating: ./data/__MACOSX/._labeled_chinese_reviews_train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd GeneratedReviewsDetection && python3 src/split.py"
      ],
      "metadata": {
        "id": "ixODk982wC56"
      },
      "id": "ixODk982wC56",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/GeneratedReviewsDetection && python3 ./src/train.py --model_version \"Langboat/mengzi-gpt-neo-base\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAB2MPPuC6to",
        "outputId": "e287a74a-ce2e-4245-847c-0689eff30ac3"
      },
      "id": "jAB2MPPuC6to",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at Langboat/mengzi-gpt-neo-base and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Backbone type: Langboat/mengzi-gpt-neo-base\n",
            "Training on device:  cuda\n",
            "Learning rate: 0.0001\n",
            "EPOCH: 1..\n",
            "Training..\n",
            "Processed 500th batch..\n",
            "Processed 1000th batch..\n",
            "Processed 1500th batch..\n",
            "Processed 2000th batch..\n",
            "Processed 2500th batch..\n",
            "Processed 3000th batch..\n",
            "Processed 3500th batch..\n",
            "Processed 4000th batch..\n",
            "Processed 4500th batch..\n",
            "Processed 5000th batch..\n",
            "Processed 5500th batch..\n",
            "Processed 6000th batch..\n",
            "Processed 6500th batch..\n",
            "Processed 7000th batch..\n",
            "Processed 7500th batch..\n",
            "Processed 8000th batch..\n",
            "Processed 8500th batch..\n",
            "Processed 9000th batch..\n",
            "Processed 9500th batch..\n",
            "Processed 10000th batch..\n",
            "Processed 10500th batch..\n",
            "Processed 11000th batch..\n",
            "Processed 11500th batch..\n",
            "Processed 12000th batch..\n",
            "Processed 12500th batch..\n",
            "Processed 13000th batch..\n",
            "Processed 13500th batch..\n",
            "Processed 14000th batch..\n",
            "Processed 14500th batch..\n",
            "Processed 15000th batch..\n",
            "Processed 15500th batch..\n",
            "Processed 16000th batch..\n",
            "Processed 16500th batch..\n",
            "Processed 17000th batch..\n",
            "Processed 17500th batch..\n",
            "Processed 18000th batch..\n",
            "Processed 18500th batch..\n",
            "Processed 19000th batch..\n",
            "Processed 19500th batch..\n",
            "Processed 20000th batch..\n",
            "Processed 20500th batch..\n",
            "Processed 21000th batch..\n",
            "Processed 21500th batch..\n",
            "Processed 22000th batch..\n",
            "Processed 22500th batch..\n",
            "Processed 23000th batch..\n",
            "Processed 23500th batch..\n",
            "Processed 24000th batch..\n",
            "Processed 24500th batch..\n",
            "Processed 25000th batch..\n",
            "Processed 25500th batch..\n",
            "Processed 26000th batch..\n",
            "Processed 26500th batch..\n",
            "Processed 27000th batch..\n",
            "Processed 27500th batch..\n",
            "Processed 28000th batch..\n",
            "Processed 28500th batch..\n",
            "Processed 29000th batch..\n",
            "Processed 29500th batch..\n",
            "Processed 30000th batch..\n",
            "Processed 30500th batch..\n",
            "Processed 31000th batch..\n",
            "Processed 31500th batch..\n",
            "Processed 32000th batch..\n",
            "Processed 32500th batch..\n",
            "Processed 33000th batch..\n",
            "Processed 33500th batch..\n",
            "Processed 34000th batch..\n",
            "Processed 34500th batch..\n",
            "Processed 35000th batch..\n",
            "Processed 35500th batch..\n",
            "Processed 36000th batch..\n",
            "Processed 36500th batch..\n",
            "Processed 37000th batch..\n",
            "Processed 37500th batch..\n",
            "Processed 38000th batch..\n",
            "Processed 38500th batch..\n",
            "Processed 39000th batch..\n",
            "Processed 39500th batch..\n",
            "Processed 40000th batch..\n",
            "Processed 40500th batch..\n",
            "Processed 41000th batch..\n",
            "Processed 41500th batch..\n",
            "Processed 42000th batch..\n",
            "Processed 42500th batch..\n",
            "Processed 43000th batch..\n",
            "Processed 43500th batch..\n",
            "Processed 44000th batch..\n",
            "Processed 44500th batch..\n",
            "Processed 45000th batch..\n",
            "Processed 45500th batch..\n",
            "Processed 46000th batch..\n",
            "Processed 46500th batch..\n",
            "Processed 47000th batch..\n",
            "Processed 47500th batch..\n",
            "Testing..\n",
            "Prediction metrics at 0.5: \n",
            "Accuracy: 0.9984080435693339\n",
            "Precision: 0.9971207087486157\n",
            "Recall: 0.9986690328305236\n",
            "\n",
            "Prediction metrics at Youden 0.2547999918460846: \n",
            "Accuracy J: 0.998659405111018\n",
            "Precision J: 0.9971226206285967\n",
            "Recall J: 0.9993345164152617\n",
            "Found best accuracy. Saving to disk.\n",
            "\n",
            "EPOCH: 2..\n",
            "Training..\n",
            "Processed 500th batch..\n",
            "Processed 1000th batch..\n",
            "Processed 1500th batch..\n",
            "Processed 2000th batch..\n",
            "Processed 2500th batch..\n",
            "Processed 3000th batch..\n",
            "Processed 3500th batch..\n",
            "Processed 4000th batch..\n",
            "Processed 4500th batch..\n",
            "Processed 5000th batch..\n",
            "Processed 5500th batch..\n",
            "Processed 6000th batch..\n",
            "Processed 6500th batch..\n",
            "Processed 7000th batch..\n",
            "Processed 7500th batch..\n",
            "Processed 8000th batch..\n",
            "Processed 8500th batch..\n",
            "Processed 9000th batch..\n",
            "Processed 9500th batch..\n",
            "Processed 10000th batch..\n",
            "Processed 10500th batch..\n",
            "Processed 11000th batch..\n",
            "Processed 11500th batch..\n",
            "Processed 12000th batch..\n",
            "Processed 12500th batch..\n",
            "Processed 13000th batch..\n",
            "Processed 13500th batch..\n",
            "Processed 14000th batch..\n",
            "Processed 14500th batch..\n",
            "Processed 15000th batch..\n",
            "Processed 15500th batch..\n",
            "Processed 16000th batch..\n",
            "Processed 16500th batch..\n",
            "Processed 17000th batch..\n",
            "Processed 17500th batch..\n",
            "Processed 18000th batch..\n",
            "Processed 18500th batch..\n",
            "Processed 19000th batch..\n",
            "Processed 19500th batch..\n",
            "Processed 20000th batch..\n",
            "Processed 20500th batch..\n",
            "Processed 21000th batch..\n",
            "Processed 21500th batch..\n",
            "Processed 22000th batch..\n",
            "Processed 22500th batch..\n",
            "Processed 23000th batch..\n",
            "Processed 23500th batch..\n",
            "Processed 24000th batch..\n",
            "Processed 24500th batch..\n",
            "Processed 25000th batch..\n",
            "Processed 25500th batch..\n",
            "Processed 26000th batch..\n",
            "Processed 26500th batch..\n",
            "Processed 27000th batch..\n",
            "Processed 27500th batch..\n",
            "Processed 28000th batch..\n",
            "Processed 28500th batch..\n",
            "Processed 29000th batch..\n",
            "Processed 29500th batch..\n",
            "Processed 30000th batch..\n",
            "Processed 30500th batch..\n",
            "Processed 31000th batch..\n",
            "Processed 31500th batch..\n",
            "Processed 32000th batch..\n",
            "Processed 32500th batch..\n",
            "Processed 33000th batch..\n",
            "Processed 33500th batch..\n",
            "Processed 34000th batch..\n",
            "Processed 34500th batch..\n",
            "Processed 35000th batch..\n",
            "Processed 35500th batch..\n",
            "Processed 36000th batch..\n",
            "Processed 36500th batch..\n",
            "Processed 37000th batch..\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GeneratedReviewsDetection/./src/train.py\", line 148, in <module>\n",
            "    train_gpt(1000)\n",
            "  File \"/content/GeneratedReviewsDetection/./src/train.py\", line 88, in train_gpt\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}