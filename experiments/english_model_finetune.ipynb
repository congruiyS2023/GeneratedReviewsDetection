{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "56b76826-a54d-47f4-ed3e-f9129d01433c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GeneratedReviewsDetection'...\n",
            "remote: Enumerating objects: 307, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 307 (delta 8), reused 12 (delta 5), pack-reused 286\u001b[K\n",
            "Receiving objects: 100% (307/307), 131.61 MiB | 15.01 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/congruiyS2023/GeneratedReviewsDetection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r GeneratedReviewsDetection/requirements.txt"
      ],
      "metadata": {
        "id": "shAjiAtJYJm1",
        "outputId": "47f6d1e1-4313-4f73-91b1-8b0531364e0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "shAjiAtJYJm1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.0 (from -r GeneratedReviewsDetection/requirements.txt (line 1))\n",
            "  Downloading transformers-4.25.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.12 (from -r GeneratedReviewsDetection/requirements.txt (line 2))\n",
            "  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==0.22 (from -r GeneratedReviewsDetection/requirements.txt (line 3))\n",
            "  Downloading scikit-learn-0.22.tar.gz (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==1.3.5 (from -r GeneratedReviewsDetection/requirements.txt (line 4))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xgboost==1.7.3 (from -r GeneratedReviewsDetection/requirements.txt (line 5))\n",
            "  Downloading xgboost-1.7.3-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.10.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.1, 0.15.2, 0.16.0, 0.16.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext==0.10.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "qmQ66zzxZEwV",
        "outputId": "808af7c6-13f5-48e4-9058-9cde8fd65c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qmQ66zzxZEwV",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.25.0"
      ],
      "metadata": {
        "id": "HZtLjNPHYT7X",
        "outputId": "a996cc3d-65c7-4752-9e83-feb34bdcdfbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HZtLjNPHYT7X",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.0\n",
            "  Using cached transformers-4.25.0-py3-none-any.whl (5.8 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.0) (2023.11.17)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.25.0 at https://files.pythonhosted.org/packages/2b/a6/b32ba581c064276dc254a4c2bdf5301a942fa28a4a9b41209d8909da37d2/transformers-4.25.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.7.0))\n",
            "Reason for being yanked: Version was not properly set\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/GeneratedReviewsDetection && mkdir data\n",
        "!cd /content/GeneratedReviewsDetection && unzip \"./Data/English Reviews/labeled_english_reviews_train.csv.zip\" -d ./data && mv ./data/labeled_english_reviews_train.csv ./data/train.csv"
      ],
      "metadata": {
        "id": "3fuHNsTFYWsH",
        "outputId": "7df282dc-a539-4536-a797-428cc0b227b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3fuHNsTFYWsH",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./Data/English Reviews/labeled_english_reviews_train.csv.zip\n",
            "  inflating: ./data/labeled_english_reviews_train.csv  \n",
            "  inflating: ./data/__MACOSX/._labeled_english_reviews_train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd GeneratedReviewsDetection && python3 utils/sanity_check.py\n",
        "!cd GeneratedReviewsDetection && python3 utils/split.py"
      ],
      "metadata": {
        "id": "csQXR0pZuPsQ",
        "outputId": "6d1833a7-c1e5-4263-c50f-7cbba791c9da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "csQXR0pZuPsQ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared 7 non-string reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/GeneratedReviewsDetection && python3 ./src/train.py"
      ],
      "metadata": {
        "id": "VVddPqwOYtd5",
        "outputId": "07378fbe-b610-4bbf-83b4-415fd58fb437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VVddPqwOYtd5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-01 18:54:15.685073: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 18:54:15.685131: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 18:54:15.685164: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 18:54:15.693152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-01 18:54:16.733076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.01k/1.01k [00:00<00:00, 4.20MB/s]\n",
            "model.safetensors: 100% 526M/526M [00:29<00:00, 17.7MB/s]\n",
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['transformer.h.11.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'score.weight', 'transformer.h.9.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Backbone type: EleutherAI/gpt-neo-125M\n",
            "Training on device:  cuda\n",
            "Learning rate: 0.0001\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 68.2MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.81MB/s]\n",
            "special_tokens_map.json: 100% 357/357 [00:00<00:00, 1.54MB/s]\n",
            "tokenizer_config.json: 100% 560/560 [00:00<00:00, 3.02MB/s]\n",
            "EPOCH: 1..\n",
            "Training..\n",
            "Processed 500th batch..\n",
            "Processed 1000th batch..\n",
            "Processed 1500th batch..\n",
            "Processed 2000th batch..\n",
            "Processed 2500th batch..\n",
            "Processed 3000th batch..\n",
            "Processed 3500th batch..\n",
            "Processed 4000th batch..\n",
            "Processed 4500th batch..\n",
            "Processed 5000th batch..\n",
            "Processed 5500th batch..\n",
            "Processed 6000th batch..\n",
            "Processed 6500th batch..\n",
            "Processed 7000th batch..\n",
            "Processed 7500th batch..\n",
            "Processed 8000th batch..\n",
            "Processed 8500th batch..\n",
            "Processed 9000th batch..\n",
            "Processed 9500th batch..\n",
            "Processed 10000th batch..\n",
            "Processed 10500th batch..\n",
            "Processed 11000th batch..\n",
            "Processed 11500th batch..\n",
            "Processed 12000th batch..\n",
            "Processed 12500th batch..\n",
            "Processed 13000th batch..\n",
            "Processed 13500th batch..\n",
            "Processed 14000th batch..\n",
            "Processed 14500th batch..\n",
            "Processed 15000th batch..\n",
            "Processed 15500th batch..\n",
            "Processed 16000th batch..\n",
            "Processed 16500th batch..\n",
            "Processed 17000th batch..\n",
            "Processed 17500th batch..\n",
            "Processed 18000th batch..\n",
            "Processed 18500th batch..\n",
            "Processed 19000th batch..\n",
            "Processed 19500th batch..\n",
            "Processed 20000th batch..\n",
            "Processed 20500th batch..\n",
            "Processed 21000th batch..\n",
            "Processed 21500th batch..\n",
            "Processed 22000th batch..\n",
            "Processed 22500th batch..\n",
            "Processed 23000th batch..\n",
            "Processed 23500th batch..\n",
            "Processed 24000th batch..\n",
            "Processed 24500th batch..\n",
            "Processed 25000th batch..\n",
            "Processed 25500th batch..\n",
            "Processed 26000th batch..\n",
            "Processed 26500th batch..\n",
            "Processed 27000th batch..\n",
            "Processed 27500th batch..\n",
            "Processed 28000th batch..\n",
            "Processed 28500th batch..\n",
            "Processed 29000th batch..\n",
            "Processed 29500th batch..\n",
            "Processed 30000th batch..\n",
            "Processed 30500th batch..\n",
            "Processed 31000th batch..\n",
            "Processed 31500th batch..\n",
            "Processed 32000th batch..\n",
            "Processed 32500th batch..\n",
            "Processed 33000th batch..\n",
            "Processed 33500th batch..\n",
            "Processed 34000th batch..\n",
            "Processed 34500th batch..\n",
            "Processed 35000th batch..\n",
            "Processed 35500th batch..\n",
            "Processed 36000th batch..\n",
            "Processed 36500th batch..\n",
            "Testing..\n",
            "Prediction metrics at 0.5: \n",
            "Accuracy: 0.9978331527627302\n",
            "Precision: 0.9965620971207564\n",
            "Recall: 0.999138302455838\n",
            "\n",
            "Prediction metrics at Youden 0.9922999739646912: \n",
            "Accuracy J: 0.9990249187432286\n",
            "Precision J: 0.9993533089027807\n",
            "Recall J: 0.998707453683757\n",
            "Found best accuracy. Saving to disk.\n",
            "\n",
            "EPOCH: 2..\n",
            "Training..\n",
            "Processed 500th batch..\n",
            "Processed 1000th batch..\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GeneratedReviewsDetection/./src/train.py\", line 148, in <module>\n",
            "    train_gpt(1000)\n",
            "  File \"/content/GeneratedReviewsDetection/./src/train.py\", line 84, in train_gpt\n",
            "    X = data['input']['input_ids'].to(device)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/GeneratedReviewsDetection && python3 ./src/evaluate.py --data_path \"./Data/English Reviews/labeled_english_reviews.csv\" --model_version 'EleutherAI/gpt-neo-125M' --weights_path \"./output/gpt-neo-125M_1.pt\""
      ],
      "metadata": {
        "id": "alBFkHcLbPm1",
        "outputId": "113e7556-ea23-4e2a-93ad-3fe23b97225e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "alBFkHcLbPm1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-01 19:50:57.263082: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 19:50:57.263159: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 19:50:57.263200: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 19:50:57.271771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-01 19:50:58.491353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading Model..\n",
            "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['transformer.h.11.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'score.weight', 'transformer.h.1.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.5.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Model Loaded.\n",
            "Evaluating..\n",
            "Evaluation metrics on test set: \n",
            "Accuracy:  99.9 %\n",
            "Precision:  99.9 %\n",
            "Recall:  99.95 %\n",
            "F1-score 99.93 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}